{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Simple baseline with Sentinel Image Patches — Swin-v2-t + Binary Cross Entropy [0.23555]\n","\n","The occurrence of different types of organisms, whether plants or animals, is generally associated with the characteristics of the environment or ecosystem in which they live. This relationship between the presence of species and their habitat is often interdependent and can be affected by various factors, such as climate, which is another modality we provide.\n","\n","To demonstrate the performance while using the image data, i.e., Sentinel Image Patches, we provide a straightforward baseline that is based on a slighly modified Swin-v2-t and Binary Cross Entropy. As described above, the satellite patches provide an image-like modality that captures habitats and other aspects of the locality.\n","\n","Considering the significant extent for enhancing performance of this baseline, we encourage you to experiment with various techniques, architectures, losses, etc.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2024-04-30T21:25:07.29831Z","start_time":"2024-04-30T21:25:05.354584Z"},"execution":{"iopub.execute_input":"2024-06-02T17:20:59.820982Z","iopub.status.busy":"2024-06-02T17:20:59.820302Z","iopub.status.idle":"2024-06-02T17:21:05.970587Z","shell.execute_reply":"2024-06-02T17:21:05.969861Z","shell.execute_reply.started":"2024-06-02T17:20:59.820952Z"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import tqdm\n","import numpy as np\n","import pandas as pd\n","import albumentations as A\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","os.chdir(\"../../\")"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-05-01T13:30:07.054038Z","iopub.status.busy":"2024-05-01T13:30:07.053659Z","iopub.status.idle":"2024-05-01T13:30:07.058148Z","shell.execute_reply":"2024-05-01T13:30:07.057269Z","shell.execute_reply.started":"2024-05-01T13:30:07.054008Z"}},"source":["## Data description\n","\n","The Sentinel Image data was acquired through the Sentinel2 satellite program and pre-processed by [Ecodatacube](https://stac.ecodatacube.eu/) to produce raster files scaled to the entire European continent and projected into a unique CRS. We filtered the data in order to pick patches from each spectral band corresponding to a location ((lon, lat) GPS coordinates) and a date matching that of our occurrences', and split them into JPEG files (RGB in 3-channels .jpeg files and NIR in single-channel .jpeg files) with a 128x128 resolution. The images were converted from sentinel uint15 to uint8 by clipping data pixel values over 10000 and applying a gamma correction of 2.5.\n","\n","The data can simply be loaded using the following method:\n","\n","```python\n","def construct_patch_path(output_path, survey_id):\n","    \"\"\"Construct the patch file path based on survey_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n","    path = output_path\n","    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n","        path = os.path.join(path, d)\n","\n","    path = os.path.join(path, f\"{survey_id}.jpeg\")\n","\n","    return path\n","```\n","\n","**References:**\n","- *Traceability (lineage): The dataset was produced entirely by mosaicking and seasonally aggregating imagery from the Sentinel-2 Level-2A product (https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/product-types/level-2a)*\n","- *Ecodatacube.eu: Analysis-ready open environmental data cube for Europe (https://doi.org/10.21203/rs.3.rs-2277090/v3)*"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare custom dataset loader\n","\n","We have to slightly update the Dataset to provide the relevant data in the appropriate format."]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2024-04-30T21:25:32.627928Z","start_time":"2024-04-30T21:25:32.612131Z"},"collapsed":false,"execution":{"iopub.execute_input":"2024-06-02T17:21:05.972579Z","iopub.status.busy":"2024-06-02T17:21:05.972183Z","iopub.status.idle":"2024-06-02T17:21:05.990264Z","shell.execute_reply":"2024-06-02T17:21:05.989424Z","shell.execute_reply.started":"2024-06-02T17:21:05.972553Z"},"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[],"source":["transform_albumentations = A.Compose([\n","    A.RandomBrightnessContrast(p=0.2),\n","    A.ColorJitter(p=0.2),\n","    A.OpticalDistortion(p=0.2)\n","])\n","\n","def construct_patch_path(data_path, survey_id):\n","    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n","    path = data_path\n","    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n","        path = os.path.join(path, d)\n","\n","    path = os.path.join(path, f\"{survey_id}.jpeg\")\n","\n","    return path\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, data_dir, metadata, transform=None):\n","        self.transform = transform\n","        self.data_dir = data_dir\n","        self.metadata = metadata\n","        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n","        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n","        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n","        \n","        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.metadata)\n","\n","    def __getitem__(self, idx):\n","        \n","        survey_id = self.metadata.surveyId[idx]\n","        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n","        num_classes = 11255\n","        label = torch.zeros(num_classes)  # Initialize label tensor\n","        for species_id in species_ids:\n","            #label_id = self.species_mapping[species_id]  # Get consecutive integer label\n","            label_id = species_id\n","            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n","        \n","        rgb_sample = np.array(Image.open(construct_patch_path(self.data_dir, survey_id)))\n","        nir_sample = np.array(Image.open(construct_patch_path(self.data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n","        \n","        \n","        rgb_sample = transform_albumentations(image=rgb_sample)\n","        nir_sample = transform_albumentations(image=nir_sample)\n","        \n","        sample = np.concatenate((rgb_sample[\"image\"], nir_sample[\"image\"][...,None]), axis=2)    \n","        sample = self.transform(sample)\n","\n","        return sample, label, survey_id\n","    \n","class TestDataset(TrainDataset):\n","    def __init__(self, data_dir, metadata, transform=None):\n","        self.transform = transform\n","        self.data_dir = data_dir\n","        self.metadata = metadata\n","        \n","    def __getitem__(self, idx):\n","        \n","        survey_id = self.metadata.surveyId[idx]\n","        \n","        rgb_sample = np.array(Image.open(construct_patch_path(self.data_dir, survey_id)))\n","        nir_sample = np.array(Image.open(construct_patch_path(self.data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n","        \n","        rgb_sample = transform_albumentations(image=rgb_sample)\n","        nir_sample = transform_albumentations(image=nir_sample)\n","        \n","        sample = np.concatenate((rgb_sample[\"image\"], nir_sample[\"image\"][...,None]), axis=2)    \n","        sample = self.transform(sample)\n","\n","        return sample, survey_id"]},{"cell_type":"markdown","metadata":{},"source":["### Load metadata and prepare data loaders"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2024-04-30T21:25:34.532017Z","start_time":"2024-04-30T21:25:32.615562Z"},"collapsed":false,"execution":{"iopub.execute_input":"2024-06-02T17:21:05.991807Z","iopub.status.busy":"2024-06-02T17:21:05.991521Z","iopub.status.idle":"2024-06-02T17:21:11.469624Z","shell.execute_reply":"2024-06-02T17:21:11.468616Z","shell.execute_reply.started":"2024-06-02T17:21:05.991784Z"},"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[],"source":["# Dataset and DataLoader\n","batch_size = 64\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","# Load Training metadata\n","train_data_path = \"/Dataset/geolifeclef-2024/PA_Train_SatellitePatches_RGB/pa_train_patches_rgb/\"\n","train_metadata_path = \"/Dataset/geolifeclef-2024/GLC24_PA_metadata_train.csv\"\n","train_metadata = pd.read_csv(train_metadata_path)\n","train_dataset = TrainDataset(train_data_path, train_metadata, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","# Load Test metadata\n","test_data_path = \"/Dataset/geolifeclef-2024/PA_Test_SatellitePatches_RGB/pa_test_patches_rgb/\"\n","test_metadata_path = \"/Dataset/geolifeclef-2024/GLC24_PA_metadata_test.csv\"\n","test_metadata = pd.read_csv(test_metadata_path)\n","test_dataset = TestDataset(test_data_path, test_metadata, transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"]},{"cell_type":"markdown","metadata":{},"source":["## Modify pretrained Swin-v2-t model\n","\n","To fully use all the R,G,B and NIR channels, we have to modify the input layer of the Swin-v2-t.\n","That is all :)"]},{"cell_type":"code","execution_count":4,"metadata":{"ExecuteTime":{"end_time":"2024-04-30T21:25:31.014067Z","start_time":"2024-04-30T21:25:31.01006Z"},"collapsed":false,"execution":{"iopub.execute_input":"2024-06-02T17:21:11.471304Z","iopub.status.busy":"2024-06-02T17:21:11.470953Z","iopub.status.idle":"2024-06-02T17:21:11.502342Z","shell.execute_reply":"2024-06-02T17:21:11.501495Z","shell.execute_reply.started":"2024-06-02T17:21:11.471271Z"},"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DEVICE = CUDA\n"]}],"source":["# Check if cuda is available\n","device = torch.device(\"cpu\")\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"DEVICE = CUDA\")\n","\n","# Hyperparameters\n","learning_rate = 0.0002\n","num_epochs = 10\n","positive_weigh_factor = 1.0\n","num_classes = 11255 # Number of all unique classes within the PO and PA data."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T17:21:11.504634Z","iopub.status.busy":"2024-06-02T17:21:11.504363Z","iopub.status.idle":"2024-06-02T17:21:14.688986Z","shell.execute_reply":"2024-06-02T17:21:14.687975Z","shell.execute_reply.started":"2024-06-02T17:21:11.504611Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/swin_v2_s-637d8ceb.pth\" to /root/.cache/torch/hub/checkpoints/swin_v2_s-637d8ceb.pth\n","100%|██████████| 191M/191M [00:01<00:00, 148MB/s]  \n"]},{"name":"stdout","output_type":"stream","text":["Adjusting learning rate of group 0 to 2.0000e-04.\n"]}],"source":["model = models.swin_v2_s(weights=\"IMAGENET1K_V1\")\n","model.features[0][0] = nn.Conv2d(4, 96, kernel_size=(4, 4), stride=(4, 4))\n","model.head = nn.Linear(in_features=768, out_features=num_classes, bias=True)\n","model.to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T17:21:14.690383Z","iopub.status.busy":"2024-06-02T17:21:14.690091Z","iopub.status.idle":"2024-06-02T17:21:14.695942Z","shell.execute_reply":"2024-06-02T17:21:14.695089Z","shell.execute_reply.started":"2024-06-02T17:21:14.690359Z"},"tags":[],"trusted":true},"outputs":[],"source":["def set_seed(seed):\n","    # Set seed for Python's built-in random number generator\n","    torch.manual_seed(seed)\n","    # Set seed for numpy\n","    np.random.seed(seed)\n","    # Set seed for CUDA if available\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","        # Set cuDNN's random number generator seed for deterministic behavior\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","set_seed(69)"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop\n","\n","Nothing special, just a standard Pytorch training loop."]},{"cell_type":"code","execution_count":7,"metadata":{"ExecuteTime":{"start_time":"2024-04-30T21:25:34.536634Z"},"collapsed":false,"execution":{"iopub.execute_input":"2024-06-02T17:21:14.697759Z","iopub.status.busy":"2024-06-02T17:21:14.697241Z","iopub.status.idle":"2024-06-02T18:43:26.582611Z","shell.execute_reply":"2024-06-02T18:43:26.581750Z","shell.execute_reply.started":"2024-06-02T17:21:14.697710Z"},"is_executing":true,"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training for 10 epochs started.\n","Epoch 1/10, Batch 0/1391, Loss: 0.6995896100997925\n","Epoch 1/10, Batch 278/1391, Loss: 0.006013783626258373\n","Epoch 1/10, Batch 556/1391, Loss: 0.0070514194667339325\n","Epoch 1/10, Batch 834/1391, Loss: 0.006077003199607134\n","Epoch 1/10, Batch 1112/1391, Loss: 0.006677394267171621\n","Epoch 1/10, Batch 1390/1391, Loss: 0.007606219500303268\n","Adjusting learning rate of group 0 to 1.9921e-04.\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 1, 'verbose': True, '_step_count': 2, '_get_lr_called_within_step': False, '_last_lr': [0.0001992114701314478]}\n","Epoch 2/10, Batch 0/1391, Loss: 0.006141382269561291\n","Epoch 2/10, Batch 278/1391, Loss: 0.006446531508117914\n","Epoch 2/10, Batch 556/1391, Loss: 0.005611842032521963\n","Epoch 2/10, Batch 834/1391, Loss: 0.006483590696007013\n","Epoch 2/10, Batch 1112/1391, Loss: 0.006265401840209961\n","Epoch 2/10, Batch 1390/1391, Loss: 0.005486813839524984\n","Adjusting learning rate of group 0 to 1.9686e-04.\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 2, 'verbose': True, '_step_count': 3, '_get_lr_called_within_step': False, '_last_lr': [0.0001968583161128631]}\n","Epoch 3/10, Batch 0/1391, Loss: 0.006948830559849739\n","Epoch 3/10, Batch 278/1391, Loss: 0.0068123359233140945\n","Epoch 3/10, Batch 556/1391, Loss: 0.008316745050251484\n","Epoch 3/10, Batch 834/1391, Loss: 0.0065798512659966946\n","Epoch 3/10, Batch 1112/1391, Loss: 0.005813613533973694\n","Epoch 3/10, Batch 1390/1391, Loss: 0.005049359053373337\n","Adjusting learning rate of group 0 to 1.9298e-04.\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 3, 'verbose': True, '_step_count': 4, '_get_lr_called_within_step': False, '_last_lr': [0.00019297764858882514]}\n","Epoch 4/10, Batch 0/1391, Loss: 0.005266929976642132\n","Epoch 4/10, Batch 278/1391, Loss: 0.005290737375617027\n","Epoch 4/10, Batch 556/1391, Loss: 0.005756215192377567\n","Epoch 4/10, Batch 834/1391, Loss: 0.005284512881189585\n","Epoch 4/10, Batch 1112/1391, Loss: 0.004866321571171284\n","Epoch 4/10, Batch 1390/1391, Loss: 0.005788566078990698\n","Adjusting learning rate of group 0 to 1.8763e-04.\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 4, 'verbose': True, '_step_count': 5, '_get_lr_called_within_step': False, '_last_lr': [0.00018763066800438636]}\n","Epoch 5/10, Batch 0/1391, Loss: 0.005728635471314192\n","Epoch 5/10, Batch 278/1391, Loss: 0.00519338296726346\n","Epoch 5/10, Batch 556/1391, Loss: 0.0051872883923351765\n","Epoch 5/10, Batch 834/1391, Loss: 0.005044552497565746\n","Epoch 5/10, Batch 1112/1391, Loss: 0.00486878352239728\n","Epoch 5/10, Batch 1390/1391, Loss: 0.004980673547834158\n","Adjusting learning rate of group 0 to 1.8090e-04.\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 5, 'verbose': True, '_step_count': 6, '_get_lr_called_within_step': False, '_last_lr': [0.00018090169943749476]}\n","Epoch 6/10, Batch 0/1391, Loss: 0.004731602035462856\n","Epoch 6/10, Batch 278/1391, Loss: 0.004544772673398256\n","Epoch 6/10, Batch 556/1391, Loss: 0.00502746319398284\n","Epoch 6/10, Batch 834/1391, Loss: 0.004750529769808054\n","Epoch 6/10, Batch 1112/1391, Loss: 0.005011697765439749\n","Epoch 6/10, Batch 1390/1391, Loss: 0.005611623637378216\n","Adjusting learning rate of group 0 to 1.7290e-04.\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 6, 'verbose': True, '_step_count': 7, '_get_lr_called_within_step': False, '_last_lr': [0.00017289686274214118]}\n","Epoch 7/10, Batch 0/1391, Loss: 0.004871261306107044\n","Epoch 7/10, Batch 278/1391, Loss: 0.005030195228755474\n","Epoch 7/10, Batch 556/1391, Loss: 0.005302327685058117\n","Epoch 7/10, Batch 834/1391, Loss: 0.004961630329489708\n","Epoch 7/10, Batch 1112/1391, Loss: 0.004759805276989937\n","Epoch 7/10, Batch 1390/1391, Loss: 0.0046533988788723946\n","Adjusting learning rate of group 0 to 1.6374e-04.\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 7, 'verbose': True, '_step_count': 8, '_get_lr_called_within_step': False, '_last_lr': [0.000163742398974869]}\n","Epoch 8/10, Batch 0/1391, Loss: 0.004350675269961357\n","Epoch 8/10, Batch 278/1391, Loss: 0.00484596798196435\n","Epoch 8/10, Batch 556/1391, Loss: 0.004523353185504675\n","Epoch 8/10, Batch 834/1391, Loss: 0.004932584706693888\n","Epoch 8/10, Batch 1112/1391, Loss: 0.004295860417187214\n","Epoch 8/10, Batch 1390/1391, Loss: 0.004201885778456926\n","Adjusting learning rate of group 0 to 1.5358e-04.\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 8, 'verbose': True, '_step_count': 9, '_get_lr_called_within_step': False, '_last_lr': [0.00015358267949789966]}\n","Epoch 9/10, Batch 0/1391, Loss: 0.004483026918023825\n","Epoch 9/10, Batch 278/1391, Loss: 0.004540813155472279\n","Epoch 9/10, Batch 556/1391, Loss: 0.004303024150431156\n","Epoch 9/10, Batch 834/1391, Loss: 0.004255068022757769\n","Epoch 9/10, Batch 1112/1391, Loss: 0.0042176684364676476\n","Epoch 9/10, Batch 1390/1391, Loss: 0.004724489524960518\n","Adjusting learning rate of group 0 to 1.4258e-04.\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 9, 'verbose': True, '_step_count': 10, '_get_lr_called_within_step': False, '_last_lr': [0.00014257792915650726]}\n","Epoch 10/10, Batch 0/1391, Loss: 0.0046220868825912476\n","Epoch 10/10, Batch 278/1391, Loss: 0.004169054329395294\n","Epoch 10/10, Batch 556/1391, Loss: 0.0048638866282999516\n","Epoch 10/10, Batch 834/1391, Loss: 0.004350182134658098\n","Epoch 10/10, Batch 1112/1391, Loss: 0.004607963375747204\n","Epoch 10/10, Batch 1390/1391, Loss: 0.004235806874930859\n","Adjusting learning rate of group 0 to 1.3090e-04.\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 10, 'verbose': True, '_step_count': 11, '_get_lr_called_within_step': False, '_last_lr': [0.00013090169943749474]}\n"]}],"source":["print(f\"Training for {num_epochs} epochs started.\")\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch_idx, (data, targets, _) in enumerate(train_loader):\n","\n","        data = data.to(device)\n","        targets = targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","\n","        pos_weight = targets*positive_weigh_factor  # All positive weights are equal to 10\n","        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","        loss = criterion(outputs, targets)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % 278 == 0:\n","            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n","\n","    scheduler.step()\n","    print(\"Scheduler:\",scheduler.state_dict())\n","\n","# Save the trained model\n","model.eval()\n","torch.save(model.state_dict(), \"models/baseline/resnet18-with-landsat-cubes.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["## Test Loop\n","\n","Again, nothing special, just a standard inference."]},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2024-06-02T18:43:26.584306Z","iopub.status.busy":"2024-06-02T18:43:26.583999Z","iopub.status.idle":"2024-06-02T18:43:58.521758Z","shell.execute_reply":"2024-06-02T18:43:58.520749Z","shell.execute_reply.started":"2024-06-02T18:43:26.584281Z"},"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 74/74 [00:31<00:00,  2.32it/s]\n"]}],"source":["with torch.no_grad():\n","    all_predictions = []\n","    surveys = []\n","    top_k_indices = None\n","    for data, surveyID in tqdm.tqdm(test_loader, total=len(test_loader)):\n","\n","        data = data.to(device)\n","        \n","        outputs = model(data)\n","        predictions = torch.sigmoid(outputs).cpu().numpy()\n","\n","        # Sellect top-25 values as predictions\n","        top_25 = np.argsort(-predictions, axis=1)[:, :25] \n","        if top_k_indices is None:\n","            top_k_indices = top_25\n","        else:\n","            top_k_indices = np.concatenate((top_k_indices, top_25), axis=0)\n","\n","        surveys.extend(surveyID.cpu().numpy())"]},{"cell_type":"markdown","metadata":{},"source":["## Save prediction file!"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T18:43:58.523538Z","iopub.status.busy":"2024-06-02T18:43:58.523232Z","iopub.status.idle":"2024-06-02T18:43:58.630451Z","shell.execute_reply":"2024-06-02T18:43:58.629533Z","shell.execute_reply.started":"2024-06-02T18:43:58.523510Z"},"tags":[],"trusted":true},"outputs":[],"source":["data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n","\n","pd.DataFrame(\n","    {'surveyId': surveys,\n","     'predictions': data_concatenated,\n","    }).to_csv(\"research/Baseline_experiments/outputs/baseline-with-sentinel-images/output.csv\", index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8171035,"sourceId":64733,"sourceType":"competition"}],"dockerImageVersionId":30716,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
